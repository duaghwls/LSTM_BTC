# ğŸ“ˆ ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„¸ ì„¤ëª…

## ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” **Encoder-Decoder LSTM** êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

## ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Input Sequence                        â”‚
â”‚              (batch, 10 days, 5 features)               â”‚
â”‚                                                          â”‚
â”‚  Features: [Open, High, Low, Close, Volume]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ENCODER LSTM                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Layer 1  â”‚â†’ â”‚ Layer 2  â”‚â†’ â”‚ Layer 3  â”‚             â”‚
â”‚  â”‚ 128 unitsâ”‚  â”‚ 128 unitsâ”‚  â”‚ 128 unitsâ”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚  Dropout: 0.2                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ Hidden State & Cell State
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DECODER LSTM                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Layer 1  â”‚â†’ â”‚ Layer 2  â”‚â†’ â”‚ Layer 3  â”‚             â”‚
â”‚  â”‚ 128 unitsâ”‚  â”‚ 128 unitsâ”‚  â”‚ 128 unitsâ”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚  Dropout: 0.2                                          â”‚
â”‚                                                         â”‚
â”‚  Autoregressive: ì´ì „ ì¶œë ¥ì„ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Fully Connected Layer                      â”‚
â”‚                (128 â†’ 5)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Output Sequence                         â”‚
â”‚              (batch, 2 days, 5 features)                â”‚
â”‚                                                          â”‚
â”‚  Only 'Close' price is used for loss calculation       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ìƒì„¸ ì„¤ëª…

### 1. Encoder (ì¸ì½”ë”)

**ì—­í• **: ê³¼ê±° 10ì¼ì˜ ê°€ê²© íŒ¨í„´ì„ ì••ì¶•ëœ hidden stateë¡œ ì¸ì½”ë”©

- **ì…ë ¥**: `(batch, 10, 5)` - 10ì¼ê°„ì˜ 5ê°œ feature
- **êµ¬ì¡°**: 3-layer LSTM, ê° layer 128 hidden units
- **ì •ê·œí™”**: Dropout 0.2
- **ì¶œë ¥**: Hidden stateì™€ Cell state (ë‹¤ìŒ ë””ì½”ë”ë¡œ ì „ë‹¬)

```python
self.encoder_lstm = nn.LSTM(
    input_size=5,        # 5ê°œ features
    hidden_size=128,     # 128 hidden units
    num_layers=3,        # 3 layers
    batch_first=True,
    dropout=0.2          # 20% dropout
)
```

### 2. Decoder (ë””ì½”ë”)

**ì—­í• **: ì¸ì½”ë”ì˜ hidden stateë¥¼ ë°›ì•„ ë¯¸ë˜ 2ì¼ì„ ìˆœì°¨ì ìœ¼ë¡œ ì˜ˆì¸¡

- **ì…ë ¥**: ì¸ì½”ë”ì˜ ë§ˆì§€ë§‰ ì¶œë ¥ (ì´ˆê¸°) + ì´ì „ ì˜ˆì¸¡ê°’ (ë°˜ë³µ)
- **êµ¬ì¡°**: 3-layer LSTM, ê° layer 128 hidden units
- **ì •ê·œí™”**: Dropout 0.2
- **ë°©ì‹**: Autoregressive (ìê¸°íšŒê·€) - ì´ì „ ì˜ˆì¸¡ì„ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©

```python
self.decoder_lstm = nn.LSTM(
    input_size=5,        # 5ê°œ features
    hidden_size=128,     # 128 hidden units
    num_layers=3,        # 3 layers
    batch_first=True,
    dropout=0.2
)
```

**Autoregressive Loop**:
```python
for t in range(2):  # 2ì¼ ì˜ˆì¸¡
    output_t, (h, c) = decoder_lstm(input_t, (h, c))
    prediction_t = fc(output_t)
    input_t = prediction_t  # ë‹¤ìŒ ì‹œì ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
```

### 3. Fully Connected Layer

**ì—­í• **: LSTM ì¶œë ¥ì„ 5ê°œ featureë¡œ ë³€í™˜

```python
self.fc = nn.Linear(128, 5)  # 128 â†’ 5
```

## ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|----------|-----|------|
| `input_seq_length` | 10 | ê³¼ê±° ëª‡ ì¼ì„ ë³¼ì§€ |
| `output_seq_length` | 2 | ë¯¸ë˜ ëª‡ ì¼ì„ ì˜ˆì¸¡í• ì§€ |
| `hidden_size` | 128 | LSTM hidden units |
| `num_layers` | 3 | LSTM ë ˆì´ì–´ ìˆ˜ |
| `dropout` | 0.2 | Dropout ë¹„ìœ¨ |
| `learning_rate` | 0.0001 | í•™ìŠµë¥  |
| `batch_size` | 32 | ë°°ì¹˜ í¬ê¸° |

## í•™ìŠµ ì „ëµ

### 1. Loss Function

**MSE (Mean Squared Error)** ì‚¬ìš©:
```python
criterion = nn.MSELoss()
loss = criterion(predicted_close, actual_close)
```

âš ï¸ **ì£¼ì˜**: ëª¨ë“  featureë¥¼ ì˜ˆì¸¡í•˜ì§€ë§Œ, **Close ê°€ê²©ë§Œ** loss ê³„ì‚°ì— ì‚¬ìš©

### 2. Optimizer

**Adam Optimizer**:
```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
```

### 3. Learning Rate Scheduling

**ReduceLROnPlateau**:
- ê²€ì¦ lossê°€ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ learning rateë¥¼ 50%ë¡œ ê°ì†Œ
- Patience: 5 epochs

```python
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.5,
    patience=5
)
```

### 4. Gradient Clipping

ê³¼ë„í•œ gradient ë°©ì§€:
```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

### 5. Early Stopping

ê³¼ì í•© ë°©ì§€:
- Patience: 30 epochs
- ê²€ì¦ lossê°€ 30 epoch ë™ì•ˆ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨
- ìµœì  ëª¨ë¸ ê°€ì¤‘ì¹˜ ìë™ ì €ì¥

## ë°ì´í„° ì „ì²˜ë¦¬

### 1. Percentage Change

ê°€ê²©ì˜ ì ˆëŒ€ê°’ ëŒ€ì‹  **ë³€í™”ìœ¨** ì‚¬ìš©:
```python
pct_change = (price_t - price_t-1) / price_t-1
```

**ì´ìœ **:
- ìŠ¤ì¼€ì¼ ë¶ˆë³€ì„±: ê°€ê²©ì´ $100ì´ë“  $100,000ì´ë“  ë™ì¼í•˜ê²Œ í•™ìŠµ
- ì •ìƒì„±(Stationarity) í–¥ìƒ

### 2. MinMax Scaling

0~1 ë²”ìœ„ë¡œ ì •ê·œí™”:
```python
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(pct_change_data)
```

### 3. ì‹œí€€ìŠ¤ ìƒì„±

ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹:
```
Day:  1  2  3  4  5  6  7  8  9 10 11 12 13
      [--------10--------] [--2--]           <- Sequence 1
         [--------10--------] [--2--]        <- Sequence 2
            [--------10--------] [--2--]     <- Sequence 3
```

## ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜

```python
Total Parameters = ì•½ 800K
```

**ê³„ì‚°**:
- Encoder LSTM: `4 Ã— ((5+128)Ã—128 + 128) Ã— 3` â‰ˆ 410K
- Decoder LSTM: `4 Ã— ((5+128)Ã—128 + 128) Ã— 3` â‰ˆ 410K
- FC Layer: `128 Ã— 5 + 5` â‰ˆ 640

## ì„±ëŠ¥ ìµœì í™” íŒ

### GPU ì‚¬ìš©

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
```

### ë°°ì¹˜ í¬ê¸° ì¡°ì •

- **GPU ë©”ëª¨ë¦¬ ì¶©ë¶„**: `batch_size = 64` ë˜ëŠ” 128
- **ë©”ëª¨ë¦¬ ë¶€ì¡±**: `batch_size = 16` ë˜ëŠ” 8

### Hidden Size ì¡°ì •

ë” ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ:
```python
hidden_size = 256  # ë˜ëŠ” 512
```

âš ï¸ ê³¼ì í•© ìœ„í—˜ ì¦ê°€

## í•œê³„ì  ë° ê°œì„  ê°€ëŠ¥ì„±

### í˜„ì¬ í•œê³„

1. **ë‹¨ì¼ ìì‚°**: BTCë§Œ ê³ ë ¤, ë‹¤ë¥¸ ìì‚°ê³¼ì˜ ìƒê´€ê´€ê³„ ë¯¸ê³ ë ¤
2. **ë‹¨ê¸° ì˜ˆì¸¡**: 2ì¼ë§Œ ì˜ˆì¸¡
3. **ì™¸ë¶€ ìš”ì¸ ë¬´ì‹œ**: ë‰´ìŠ¤, ì •ì±… ë“± ë°˜ì˜ ì•ˆ ë¨

### ê°œì„  ì•„ì´ë””ì–´

1. **Attention Mechanism** ì¶”ê°€
2. **Transformer** ì•„í‚¤í…ì²˜ ë„ì…
3. **Multi-variate Input**: ETH, S&P500 ë“± ì¶”ê°€ ìì‚° í¬í•¨
4. **Sentiment Analysis**: ë‰´ìŠ¤/ì†Œì…œë¯¸ë””ì–´ ê°ì„± ë¶„ì„ ì¶”ê°€
5. **Ensemble**: ì—¬ëŸ¬ ëª¨ë¸ ì¡°í•©

## ì°¸ê³  ìë£Œ

- [LSTM ë…¼ë¬¸ (Hochreiter & Schmidhuber, 1997)](https://www.bioinf.jku.at/publications/older/2604.pdf)
- [Sequence to Sequence Learning (Sutskever et al., 2014)](https://arxiv.org/abs/1409.3215)
- [PyTorch LSTM Documentation](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)
