import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import time
import copy

# tqdmì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ë°˜ë³µë¬¸ ì‚¬ìš©
try:
    from tqdm import tqdm

    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    print(
        "tqdmì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. 'pip install tqdm'ìœ¼ë¡œ ì„¤ì¹˜í•˜ë©´ ì§„í–‰ ìƒí™© í‘œì‹œê°€ ë” ìì„¸í•´ì§‘ë‹ˆë‹¤."
    )

df = pd.read_csv("../data/BTC-USD.csv")
df["Date"] = pd.to_datetime(df["Date"])

train_data = df[df["Date"] <= "2024-11-12"]
test_data = df[df["Date"] > "2024-11-12"]

train_data = train_data.drop(columns=["Date"])
test_data = test_data.drop(columns=["Date"])

# differencing the data
train_data_pct = train_data.copy()
test_data_pct = test_data.copy()
for col in train_data.columns:
    train_data_pct[col] = train_data[col].pct_change()
    test_data_pct[col] = test_data[col].pct_change()

train_data_pct = train_data_pct.dropna()
test_data_pct = test_data_pct.dropna()


# ì •ê·œí™” ì ìš©
scaler = MinMaxScaler()
train_data_pct = scaler.fit_transform(train_data_pct)
test_data_pct = scaler.transform(test_data_pct)


# ì‹œí€€ìŠ¤ ìƒì„± (many to many) - í•™ìŠµ/ì˜ˆì¸¡ ìœˆë„ìš° ë¶„ë¦¬
def create_sequences(data, input_seq_length, output_seq_length):
    X, y = [], []
    for i in range(input_seq_length, len(data) - output_seq_length + 1):
        X.append(data[i - input_seq_length : i])  # ê³¼ê±° input_seq_lengthì¼
        y.append(
            data[i : i + output_seq_length, 0]
        )  # ë¯¸ë˜ output_seq_lengthì¼ì˜ Closeë§Œ
    return np.array(X), np.array(y)


input_seq_length = 10  # ê³¼ê±° 10ì¼ í•™ìŠµ
output_seq_length = 2  # ë¯¸ë˜ 2ì¼ ì˜ˆì¸¡

train_X_pct, train_y_pct = create_sequences(
    train_data_pct, input_seq_length, output_seq_length
)
test_X_pct, test_y_pct = create_sequences(
    np.concatenate([train_data_pct, test_data_pct]), input_seq_length, output_seq_length
)

# Train/Validation ë¶„í•  (80/20)
train_X_final, val_X, train_y_final, val_y = train_test_split(
    train_X_pct, train_y_pct, test_size=0.2, shuffle=False
)


# Early Stopping í´ë˜ìŠ¤
class EarlyStopping:
    def __init__(self, patience=10, min_delta=0, verbose=True):
        self.patience = patience
        self.min_delta = min_delta
        self.verbose = verbose
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
        self.best_model = None

    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.best_model = copy.deepcopy(model.state_dict())
            if self.verbose:
                print(f"  âœ“ ê²€ì¦ ì†ì‹¤ ì´ˆê¸°í™”: {val_loss:.6f}")
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.verbose:
                print(f"  âš  EarlyStopping ì¹´ìš´í„°: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
                if self.verbose:
                    print(
                        f"  ğŸ›‘ Early Stopping ë°œë™! ìµœì  ê²€ì¦ ì†ì‹¤: {self.best_loss:.6f}"
                    )
        else:
            improvement = self.best_loss - val_loss
            self.best_loss = val_loss
            self.best_model = copy.deepcopy(model.state_dict())
            self.counter = 0
            if self.verbose:
                print(f"  âœ“ ê²€ì¦ ì†ì‹¤ ê°œì„ : {improvement:.6f} â†’ ìµœì  ëª¨ë¸ ì €ì¥")


# ë‹¤ë³€ëŸ‰ LSTM ëª¨ë¸ ì •ì˜
class MultivariateLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, seq_length):
        super(MultivariateLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.seq_length = seq_length
        self.input_size = input_size

        # Encoder LSTM
        self.encoder_lstm = nn.LSTM(
            input_size, hidden_size, num_layers, batch_first=True, dropout=0.2
        )

        # Decoder LSTM
        self.decoder_lstm = nn.LSTM(
            input_size, hidden_size, num_layers, batch_first=True, dropout=0.2
        )

        # Output layer - ëª¨ë“  feature ì˜ˆì¸¡
        self.fc = nn.Linear(hidden_size, input_size)

    def forward(self, x):
        # Encoder
        encoder_output, (hidden, cell) = self.encoder_lstm(x)

        # Decoder - ì²« ë²ˆì§¸ ì…ë ¥ì€ ì¸ì½”ë”ì˜ ë§ˆì§€ë§‰ ì¶œë ¥
        decoder_input = x[:, -1:, :]  # (batch, 1, input_size)
        decoder_outputs = []

        for _ in range(self.seq_length):
            decoder_output, (hidden, cell) = self.decoder_lstm(
                decoder_input, (hidden, cell)
            )
            output = self.fc(
                decoder_output
            )  # (batch, 1, input_size) - ëª¨ë“  feature ì˜ˆì¸¡
            decoder_outputs.append(output)
            decoder_input = output  # ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš© (ì°¨ì› ì¼ì¹˜!)

        decoder_outputs = torch.cat(
            decoder_outputs, dim=1
        )  # (batch, seq_length, input_size)
        return decoder_outputs


# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
input_size = train_X_pct.shape[2]  # íŠ¹ì„± ê°œìˆ˜
hidden_size = 128
num_layers = 3
learning_rate = 0.0001
num_epochs = 200
batch_size = 32
patience = 30  # Early Stopping patience
max_grad_norm = 1.0  # Gradient clipping

# ëª¨ë¸ ì´ˆê¸°í™”
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = MultivariateLSTM(input_size, hidden_size, num_layers, output_seq_length).to(
    device
)

# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Learning Rate Scheduler
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode="min", factor=0.5, patience=5
)

# Early Stopping ì´ˆê¸°í™”
early_stopping = EarlyStopping(patience=patience, min_delta=1e-6, verbose=True)

# ë°ì´í„°ë¥¼ í…ì„œë¡œ ë³€í™˜
train_X_tensor = torch.FloatTensor(train_X_final).to(device)
train_y_tensor = torch.FloatTensor(train_y_final).to(device)
val_X_tensor = torch.FloatTensor(val_X).to(device)
val_y_tensor = torch.FloatTensor(val_y).to(device)

# DataLoader ìƒì„±
train_dataset = torch.utils.data.TensorDataset(train_X_tensor, train_y_tensor)
train_loader = torch.utils.data.DataLoader(
    train_dataset, batch_size=batch_size, shuffle=True
)
val_dataset = torch.utils.data.TensorDataset(val_X_tensor, val_y_tensor)
val_loader = torch.utils.data.DataLoader(
    val_dataset, batch_size=batch_size, shuffle=False
)

# í•™ìŠµ
print("=" * 60)
print(f"í•™ìŠµ ì‹œì‘ - Device: {device}")
print(f"Train X shape: {train_X_final.shape}, Train y shape: {train_y_final.shape}")
print(f"Val X shape: {val_X.shape}, Val y shape: {val_y.shape}")
print(f"ì´ í›ˆë ¨ ë°°ì¹˜ ìˆ˜: {len(train_loader)}, ê²€ì¦ ë°°ì¹˜ ìˆ˜: {len(val_loader)}")
print(f"Early Stopping Patience: {patience}, Max Gradient Norm: {max_grad_norm}")
print("=" * 60)

train_losses = []
val_losses = []
start_time = time.time()

for epoch in range(num_epochs):
    # ========== í›ˆë ¨ ë‹¨ê³„ ==========
    model.train()
    epoch_loss = 0
    epoch_start_time = time.time()

    # ë°°ì¹˜ ì§„í–‰ ìƒí™© í‘œì‹œ
    if HAS_TQDM:
        pbar = tqdm(
            train_loader,
            desc=f"Epoch [{epoch+1}/{num_epochs}] Train",
            ncols=100,
            leave=False,
        )
        batch_iter = pbar
    else:
        batch_iter = train_loader

    for batch_idx, (batch_X, batch_y) in enumerate(batch_iter):
        # Forward pass - Close ê°’ë§Œ ì†ì‹¤ ê³„ì‚°
        outputs = model(batch_X)  # (batch, seq_length, input_size)
        loss = criterion(outputs[:, :, 0], batch_y)  # Closeë§Œ ì†ì‹¤ ê³„ì‚°

        # Backward pass
        optimizer.zero_grad()
        loss.backward()

        # Gradient Clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)

        optimizer.step()

        epoch_loss += loss.item()

        # ë°°ì¹˜ë³„ ì†ì‹¤ í‘œì‹œ
        if HAS_TQDM:
            pbar.set_postfix({"Loss": f"{loss.item():.6f}"})
        elif (batch_idx + 1) % max(1, len(train_loader) // 10) == 0:
            # tqdmì´ ì—†ìœ¼ë©´ 10%ë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
            progress = (batch_idx + 1) / len(train_loader) * 100
            print(
                f"  ë°°ì¹˜ ì§„í–‰: {batch_idx+1}/{len(train_loader)} ({progress:.1f}%) - Loss: {loss.item():.6f}",
                end="\r",
            )

    avg_train_loss = epoch_loss / len(train_loader)
    train_losses.append(avg_train_loss)

    # ========== ê²€ì¦ ë‹¨ê³„ ==========
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for batch_X, batch_y in val_loader:
            outputs = model(batch_X)
            loss = criterion(outputs[:, :, 0], batch_y)  # Closeë§Œ ì†ì‹¤ ê³„ì‚°
            val_loss += loss.item()

    avg_val_loss = val_loss / len(val_loader)
    val_losses.append(avg_val_loss)

    # Learning Rate Scheduler
    prev_lr = optimizer.param_groups[0]["lr"]
    scheduler.step(avg_val_loss)
    current_lr = optimizer.param_groups[0]["lr"]

    # LR ë³€ê²½ ê°ì§€ ë° ì¶œë ¥
    lr_changed = prev_lr != current_lr
    lr_info = ""
    if lr_changed:
        lr_info = f" | âš¡ LR ê°ì†Œ: {prev_lr:.6f} â†’ {current_lr:.6f}"

    # ì—í­ë³„ ìƒì„¸ ì •ë³´ ì¶œë ¥
    epoch_time = time.time() - epoch_start_time
    elapsed_time = time.time() - start_time
    avg_time_per_epoch = elapsed_time / (epoch + 1)
    remaining_time = avg_time_per_epoch * (num_epochs - epoch - 1)

    print(
        f"Epoch [{epoch+1:3d}/{num_epochs}] | "
        f"Train Loss: {avg_train_loss:.6f} | "
        f"Val Loss: {avg_val_loss:.6f} | "
        f"LR: {current_lr:.6f}{lr_info} | "
        f"ì‹œê°„: {epoch_time:.1f}ì´ˆ"
    )

    # Early Stopping ì²´í¬
    early_stopping(avg_val_loss, model)
    if early_stopping.early_stop:
        print(f"\nğŸ›‘ Early Stopping ë°œë™! (Epoch {epoch+1})")
        # ìµœì  ëª¨ë¸ ë³µì›
        model.load_state_dict(early_stopping.best_model)
        break

# ëª¨ë¸ ì €ì¥
total_time = time.time() - start_time
actual_epochs = len(train_losses)

# ìµœì  ëª¨ë¸ ì €ì¥ (Early Stoppingì´ best modelì„ ì´ë¯¸ ë¡œë“œí•¨)
torch.save(model.state_dict(), "../outputs/models/lstm_btc_model_best.pth")

print("\n" + "=" * 60)
print("í•™ìŠµ ì™„ë£Œ!")
print(f"ì‹¤í–‰ëœ ì—í­: {actual_epochs}/{num_epochs}")
print(f"ì´ ì†Œìš” ì‹œê°„: {total_time/60:.2f}ë¶„ ({total_time:.2f}ì´ˆ)")
print(f"í‰ê·  ì—í­ë‹¹ ì‹œê°„: {total_time/actual_epochs:.2f}ì´ˆ")
print(f"ìµœì¢… í›ˆë ¨ ì†ì‹¤: {train_losses[-1]:.6f}")
print(f"ìµœì¢… ê²€ì¦ ì†ì‹¤: {val_losses[-1]:.6f}")
print(f"ìµœê³  ì„±ëŠ¥ (ìµœì € ê²€ì¦ ì†ì‹¤): {early_stopping.best_loss:.6f}")
print(f"ìµœì  ëª¨ë¸ ì €ì¥ ì™„ë£Œ: outputs/models/lstm_btc_model_best.pth")
print("=" * 60)

# í•™ìŠµ ë° ê²€ì¦ ì†ì‹¤ ì‹œê°í™”
plt.figure(figsize=(12, 6))

# ì„œë¸Œí”Œë¡¯ 1: í›ˆë ¨ vs ê²€ì¦ ì†ì‹¤
plt.subplot(1, 2, 1)
plt.plot(train_losses, label="Train Loss", linewidth=2)
plt.plot(val_losses, label="Validation Loss", linewidth=2)
plt.title("Training vs Validation Loss", fontsize=14, fontweight="bold")
plt.xlabel("Epoch", fontsize=12)
plt.ylabel("Loss", fontsize=12)
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)

# ì„œë¸Œí”Œë¡¯ 2: ì†ì‹¤ ì°¨ì´ (ê³¼ì í•© ëª¨ë‹ˆí„°ë§)
plt.subplot(1, 2, 2)
loss_diff = np.array(val_losses) - np.array(train_losses)
plt.plot(loss_diff, color="red", linewidth=2)
plt.axhline(y=0, color="black", linestyle="--", alpha=0.5)
plt.title(
    "Validation - Train Loss (Overfitting Monitor)", fontsize=14, fontweight="bold"
)
plt.xlabel("Epoch", fontsize=12)
plt.ylabel("Loss Difference", fontsize=12)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig("../outputs/figures/training_validation_loss.png", dpi=100, bbox_inches="tight")
print("í•™ìŠµ/ê²€ì¦ ì†ì‹¤ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: outputs/figures/training_validation_loss.png")
plt.close()

# ì¶”ê°€: Learning Rate ë³€í™” ì‹œê°í™” (ì„ íƒì )
print("\ní•™ìŠµ ìš”ì•½:")
print(f"  - ì´ˆê¸° Learning Rate: {learning_rate}")
print(f"  - ìµœì¢… Learning Rate: {optimizer.param_groups[0]['lr']:.8f}")
print(
    f"  - Early Stopping Triggered: {'Yes (Epoch ' + str(actual_epochs) + ')' if actual_epochs < num_epochs else 'No'}"
)
